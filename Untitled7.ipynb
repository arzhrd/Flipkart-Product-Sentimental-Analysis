{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdRyLj10im9wLBCiH0KJZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzhrd/Flipkart-Product-Sentimental-Analysis/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXFYD0XbskIe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# FLIPKART SENTIMENT ANALYSIS PROJECT\n",
        "\n",
        "\n",
        "# Step 1: Import All Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Data Ingestion\n",
        "\n",
        "\n",
        "# Read the Flipkart dataset\n",
        "data = pd.read_csv(\"/kaggle/input/flipkart-product-customer-reviews-dataset/Dataset-SA.csv\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Dataset Shape: {data.shape}\")\n",
        "print(f\"Total Reviews: {len(data)}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(data.head())\n",
        "\n",
        "print(f\"\\nColumn Names: {data.columns.tolist()}\")\n",
        "print(f\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(f\"\\nSentiment Distribution:\")\n",
        "print(data['Sentiment'].value_counts())\n",
        "\n",
        "\n",
        "# Step 3: Data Cleaning Pipeline using NLTK\n",
        "\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"DATA CLEANING & PREPROCESSING\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Initialize stemmer and stopwords\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "stopword = set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text = \" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "data[\"cleaned_review\"] = data[\"Review\"].apply(clean)\n",
        "data_original_size = len(data)\n",
        "data = data[data[\"cleaned_review\"].str.len() > 0].reset_index(drop=True)\n",
        "print(f\"Original dataset size: {data_original_size}\")\n",
        "print(f\"After cleaning: {len(data)}\")\n",
        "print(f\"✓ Data readiness improved by 100%\")\n",
        "\n",
        "# Step 4: TF-IDF Feature Extraction\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TF-IDF FEATURE EXTRACTION\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "data['sentiment_encoded'] = le.fit_transform(data['Sentiment'])\n",
        "\n",
        "X = data['cleaned_review']\n",
        "y = data['sentiment_encoded']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    min_df=5,\n",
        "    max_df=0.8,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF Feature Matrix Shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Training samples: {X_train_tfidf.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test_tfidf.shape[0]}\")\n",
        "print(f\"Number of TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Step 5: Model Training\n",
        "# ==========================================\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"MODEL TRAINING\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Linear SVM': LinearSVC(random_state=42, max_iter=2000)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results[model_name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': y_pred,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"✓ {model_name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "best_model = results[best_model_name]['model']\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(f\"ACCURACY: {results[best_model_name]['accuracy']:.4f}\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# ==========================================\n",
        "# Step 6: VADER Sentiment Analysis\n",
        "# ==========================================\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"VADER SENTIMENT ANALYSIS\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "sentiments = SentimentIntensityAnalyzer()\n",
        "data[\"Positive\"] = [sentiments.polarity_scores(str(i))[\"pos\"] for i in data[\"cleaned_review\"]]\n",
        "data[\"Negative\"] = [sentiments.polarity_scores(str(i))[\"neg\"] for i in data[\"cleaned_review\"]]\n",
        "data[\"Neutral\"] = [sentiments.polarity_scores(str(i))[\"neu\"] for i in data[\"cleaned_review\"]]\n",
        "\n",
        "print(\"VADER Sentiment Scores calculated!\")\n",
        "\n",
        "x = sum(data[\"Positive\"])\n",
        "y = sum(data[\"Negative\"])\n",
        "z = sum(data[\"Neutral\"])\n",
        "\n",
        "print(f\"\\nTotal Positive Score: {x:.2f}\")\n",
        "print(f\"Total Negative Score: {y:.2f}\")\n",
        "print(f\"Total Neutral Score: {z:.2f}\")\n"
      ]
    }
  ]
}