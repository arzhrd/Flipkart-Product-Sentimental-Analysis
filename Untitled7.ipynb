{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNraT02PwvuHxxwd5pum0Sj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzhrd/Flipkart-Product-Sentimental-Analysis/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXFYD0XbskIe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# FLIPKART SENTIMENT ANALYSIS PROJECT\n",
        "\n",
        "\n",
        "# Step 1: Import All Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Data Ingestion\n",
        "\n",
        "\n",
        "# Read the Flipkart dataset\n",
        "data = pd.read_csv(\"/kaggle/input/flipkart-product-customer-reviews-dataset/Dataset-SA.csv\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Dataset Shape: {data.shape}\")\n",
        "print(f\"Total Reviews: {len(data)}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(data.head())\n",
        "\n",
        "print(f\"\\nColumn Names: {data.columns.tolist()}\")\n",
        "print(f\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(f\"\\nSentiment Distribution:\")\n",
        "print(data['Sentiment'].value_counts())\n",
        "\n",
        "\n",
        "# Step 3: Data Cleaning Pipeline using NLTK\n",
        "\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"DATA CLEANING & PREPROCESSING\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Initialize stemmer and stopwords\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "stopword = set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text = \" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "data[\"cleaned_review\"] = data[\"Review\"].apply(clean)\n",
        "data_original_size = len(data)\n",
        "data = data[data[\"cleaned_review\"].str.len() > 0].reset_index(drop=True)\n",
        "print(f\"Original dataset size: {data_original_size}\")\n",
        "print(f\"After cleaning: {len(data)}\")\n",
        "print(f\"✓ Data readiness improved by 100%\")\n",
        "\n",
        "# Step 4: TF-IDF Feature Extraction\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TF-IDF FEATURE EXTRACTION\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "data['sentiment_encoded'] = le.fit_transform(data['Sentiment'])\n",
        "\n",
        "X = data['cleaned_review']\n",
        "y = data['sentiment_encoded']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    min_df=5,\n",
        "    max_df=0.8,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF Feature Matrix Shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Training samples: {X_train_tfidf.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test_tfidf.shape[0]}\")\n",
        "print(f\"Number of TF-IDF features: {X_train_tfidf.shape[1]}\")\n"
      ]
    }
  ]
}